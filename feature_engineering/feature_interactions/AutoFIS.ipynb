{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<script type=\"text/javascript\" src=\"http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default\"></script>\n",
    "\n",
    "AutoFIS: Automatic Feature Interaction Selection in Factorization Models for Click-Through Rate Prediction\n",
    "\n",
    "\n",
    "\n",
    "# Proceduces: Two Stages\n",
    "\n",
    "## 0. Background\n",
    "\n",
    "<img src='images/autofis_background.jpg'>\n",
    "\n",
    "\n",
    "\n",
    "Raw factorization machines based models.\n",
    "\n",
    "+ Input layer (embedding): $E = [e_1, e_2, ..., e_m]$\n",
    "\n",
    "+ Feature interaction layer: $[<e_1, e_2>, <e_1, e_3>, ..., <e_{m-1}, e_m>]$\n",
    "\n",
    "$l_{fm}=<\\textbf{w}, \\textbf{x}> + \\sum_{i=1}^m\\sum_{j>i}^m <e_i, e_j>$\n",
    "\n",
    "+ MLP layer: $a^{l+1}=relu(W^{(l)}a^{(l)}+b^{(l)})$\n",
    "+ Output layer: $\\hat{y}_{FM}=sigmoid(l_{fm})=\\frac{1}{1+exp(-l_{fm})}$\n",
    "\n",
    "DeepFM: $\\hat{y}_{DeepFM}=sigmoid(l_{fm}+MLP(E))$\n",
    "\n",
    "IPNN: $\\hat{y}_{IPNN}=sigmoid(MLP([E, l_{fm}]))$\n",
    "\n",
    "\n",
    "\n",
    "## 1. Search Stage\n",
    "\n",
    "+ **Gate****: Use a **gate** operation.\n",
    "\n",
    "<img src='images/autofis_overview.jpg'>\n",
    "\n",
    "\n",
    "\n",
    "$l_{AutoFIS}=<\\textbf{w}, \\textbf{x}> + \\sum_{i=1}^m \\sum{j>i}^m \\alpha (i, j) <e_i, e_j>$\n",
    "\n",
    "$\\textbf{\\alpha}=\\{\\alpha_{(1, 2)}, \\alpha_{(1, 3)}, ..., \\alpha_{(m-1, m)}\\}$ are the artitechture parameters.\n",
    "\n",
    "\n",
    "\n",
    "+ Batch Normalization\n",
    "\n",
    "Since $<e_i, e_j> \\cdot \\alpha_{(i, j)}$ represtents the conbution of interaction pair $e_i$ and $e_j$, and they are jointly learned, BN is used on $<e_i, e_j>$ to eliminate the scale issue.\n",
    "\n",
    "+ GRDA Optimizer\n",
    "\n",
    "Use generalized regularized dual averaging (GRDA) optimizer to learn a sparse DNN for $\\textbf{\\alpha}$ while other parameters are learned by Adam optimizer as normal.\n",
    "\n",
    "$\\alpha_{t+1}=\\arg\\min_{\\alpha}\\{ \\alpha^T (-\\alpha_0 + \\gamma \\sum_{i=0}^t) \\nabla L(\\alpha_t;Z_{i+1}) + g(t, \\gamma) ||\\alpha||_1 + 1/2 ||\\alpha||_2^2 \\}$\n",
    "\n",
    "## 2. Retrain Stage\n",
    "\n",
    "Let $G(i, j)$ represents the gate status of feature interaction $<e_i, e_j>$, set $G(i, j)$ as 0 when $\\alpha_{(i,j)}^{*}=0$; otherwise $G(i,j)$ is set to 1.\n",
    "\n",
    "Remove the unimportant interactions, retrain the model with $\\textbf{\\alpha}$ kept as attetion weight.\n",
    "\n",
    "$l_{fm}^{re} = <\\textbf{w}, \\textbf{x}> + \\sum_{i=1}^m \\sum_{j>i}^m \\alpha_{(i,j)} G(i,j) <e_i, e_j>$\n",
    "\n",
    "\n",
    "\n",
    "# Analysis\n",
    "\n",
    "## 1. Advantage\n",
    "\n",
    "+ Use two stages (search and retrain) to select effective interaction pairs, which not only brings a better performance, but also leads engineers to analysis why these pairs are much more importanct.\n",
    "+ Show that removing some useless interaction pairs, the performance may be **improved**.\n",
    "+ May reduce the model and feature storage, and inference resources.\n",
    "\n",
    "\n",
    "\n",
    "## 2. Disadvantage\n",
    "\n",
    "+ Need to train models two times, which is hard for online learning.\n",
    "+ May cause much more inference sources than pure FM (for the expansion of interaction pairs).\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
